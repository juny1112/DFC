import pandas as pd
import numpy as np
import os
from tqdm import tqdm
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¶©ì „í›„ êµ¬ê°„ ë¶ˆí•„ìš”í•œ ë°ì´í„° ì‚­ì œ (ë²¡í„°í™”, ì¸ë±ìŠ¤ ì•ˆì „)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def remove_consecutive_ones(data):
    if 'R_aftercharg' not in data.columns:
        return data

    s = data['R_aftercharg'].fillna(0).astype(int)
    grp = (s != s.shift(fill_value=s.iloc[0])).cumsum()          # ì—°ì† êµ¬ê°„ ë¼ë²¨
    group_sizes = grp.map(grp.value_counts())                    # ê° í–‰ì´ ì†í•œ êµ¬ê°„ ê¸¸ì´
    pos_from_start = data.groupby(grp).cumcount()
    pos_from_end = data.iloc[::-1].groupby(grp.iloc[::-1]).cumcount()[::-1]

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â‘  ë³´í˜¸ ëŒ€ìƒ R_aftercharg ê·¸ë£¹ ì°¾ê¸°
    #    ì¡°ê±´: í•´ë‹¹ after êµ¬ê°„ ë°”ë¡œ ì§ì „ R_charg êµ¬ê°„ì˜ "ì‹œì‘ SOC â‰¥ 95"
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    protect_groups = set()

    if ('R_charg' in data.columns) and ('soc' in data.columns):
        # R_aftercharg == 1 ì¸ ê·¸ë£¹ë“¤ë§Œ ëŒ€ìƒ
        after_groups = grp[s == 1].unique()

        for g in after_groups:
            # ì´ ê·¸ë£¹ì— ì†í•œ ì¸ë±ìŠ¤ë“¤
            idxs = np.flatnonzero(grp.values == g)
            if len(idxs) == 0:
                continue

            start_idx = int(idxs[0])  # after êµ¬ê°„ ì‹œì‘ í–‰ ì¸ë±ìŠ¤

            # ë§¨ ì•ì´ë©´ ë°”ë¡œ ì§ì „ êµ¬ê°„ì´ ì—†ìœ¼ë¯€ë¡œ íŒ¨ìŠ¤
            if start_idx == 0:
                continue

            prev_idx = start_idx - 1

            # ì§ì „ í–‰ì´ R_charg==1 ì´ ì•„ë‹ˆë©´ íŒ¨ìŠ¤
            rc_prev = int(data.loc[prev_idx, 'R_charg']) if not pd.isna(data.loc[prev_idx, 'R_charg']) else 0
            if rc_prev != 1:
                continue

            # ì§ì „ R_charg êµ¬ê°„ì˜ "ì‹œì‘ ì¸ë±ìŠ¤" ì°¾ê¸° (ì—°ì† 1 êµ¬ê°„ì˜ ë§¨ ì•)
            k = prev_idx
            while k > 0:
                val = data.loc[k-1, 'R_charg']
                rc = int(val) if not pd.isna(val) else 0
                if rc != 1:
                    break
                k -= 1
            start_charg_idx = k

            # ê·¸ êµ¬ê°„ ì‹œì‘ ì‹œì  SOC â‰¥ 95 ì¸ì§€ í™•ì¸
            soc_start = pd.to_numeric(data.loc[start_charg_idx, 'soc'], errors='coerce')
            if pd.notna(soc_start) and soc_start >= 95:
                protect_groups.add(g)

    # ê° í–‰ì´ ë³´í˜¸ ëŒ€ìƒ ê·¸ë£¹ì¸ì§€ ì—¬ë¶€ ì‹œë¦¬ì¦ˆ
    protect_flag = grp.isin(protect_groups)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â‘¡ keep ë§ˆìŠ¤í¬ êµ¬ì„±
    #    - s==0: í•­ìƒ ìœ ì§€
    #    - s==1 & ë³´í˜¸ ê·¸ë£¹: ì „ë¶€ ìœ ì§€ (ì¤‘ê°„í–‰ ì‚­ì œ ê¸ˆì§€)
    #    - s==1 & ë¹„ë³´í˜¸ ê·¸ë£¹:
    #        Â· ê¸¸ì´<3 â†’ ì „ë¶€ ìœ ì§€
    #        Â· ê¸¸ì´â‰¥3 â†’ ì²˜ìŒ/ë§ˆì§€ë§‰ë§Œ ìœ ì§€
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    keep = (
        (s == 0) |
        ((s == 1) & protect_flag) |
        ((s == 1) & ~protect_flag & (group_sizes < 3)) |
        ((s == 1) & ~protect_flag & (group_sizes >= 3) & ((pos_from_start == 0) | (pos_from_end == 0)))
    )

    return data.loc[keep].reset_index(drop=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DFC ì•Œê³ ë¦¬ì¦˜ ì ìš© + ì´ë²¤íŠ¸ í†µê³„(ì„ íƒ)
#   - ì‹¤ì œ time shiftê°€ ë°œìƒí•œ ì´ë²¤íŠ¸ë“¤ì˜ delayed_timeì„ ìˆ˜ì§‘
#   - collect_stats=Trueì¼ ë•Œ (events ë¦¬ìŠ¤íŠ¸, ìš”ì•½ dict)ë„ í•¨ê»˜ ë°˜í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def DFC(data, collect_stats=False):
    data = remove_consecutive_ones(data)

    # time íŒŒì‹± ì•ˆì „í™”
    if 'time' in data.columns and not pd.api.types.is_datetime64_any_dtype(data['time']):
        data['time'] = pd.to_datetime(data['time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')

    # â”€ ì¶©ì „ êµ¬ê°„ ê²½ê³„ ìˆ˜ì§‘
    charg = []
    if data.loc[0, 'R_charg'] == 1:
        charg.append(0)
    for i in range(len(data) - 1):
        if data.loc[i, 'R_charg'] != data.loc[i + 1, 'R_charg']:
            charg.append(i + 1)
    if data.loc[len(data) - 1, 'R_charg'] == 1:
        charg.append(len(data) - 1)

    # cendÂ±1 ê·¼ì ‘ ì²´í¬
    def any_after_near(idx):
        lo = max(0, idx - 1)
        hi = min(len(data) - 1, idx + 1)
        return (data.loc[lo:hi, 'R_aftercharg'] == 1).any()

    # DFC ì ìš© ì¶©ì „êµ¬ê°„
    dfc_charg = []
    for i in range(len(charg) - 1):
        if data.loc[charg[i], 'R_charg'] == 1 and any_after_near(charg[i + 1] - 1):
            dfc_charg.append(charg[i])
            dfc_charg.append(charg[i + 1] - 1)

    # â”€ ì¶©ì „ë‹¨ê³„2: ì •í™•íˆ 80 ê¸°ì¤€ + ìŒ ìœ ì§€
    charg_2_pairs = []   # (delay_start, charge_end)
    delay_start = []
    for j in range(0, len(dfc_charg) - 1, 2):
        start_j, end_j = dfc_charg[j], dfc_charg[j + 1]
        found = False
        # (1) êµ¬ê°„ ë‚´ë¶€ì—ì„œ 79â†’80%ê°€ ë˜ëŠ” ìˆœê°„ì„ ì°¾ìœ¼ë©´ ê·¸ ë‹¤ìŒ ì¸ë±ìŠ¤ë¥¼ ì§€ì—° ì‹œì‘ìœ¼ë¡œ ì‚¬ìš©
        for i in range(start_j, max(start_j, end_j)):  # [start_j, end_j-1]
            if (data.loc[i, 'soc'] < 80) and (data.loc[i + 1, 'soc'] == 80):
                charg_2_pairs.append((i + 1, end_j))
                delay_start.append(i + 1)
                found = True
                break
        # (2) ìœ„ê°€ ì—†ê³ , **ì¶©ì „ ì‹œì‘ SOCê°€ 80% ì´ìƒì´ë©´ ì¶©ì „ ì‹œì‘ ì‹œì ë¶€í„° ì§€ì—° ì‹œì‘**
        if (not found) and (data.loc[start_j, 'soc'] >= 80):
            charg_2_pairs.append((start_j, end_j))
            delay_start.append(start_j)

    # # â”€ aftercharge ì¢…ë£Œì (ì›í˜•)
    # after = []
    # if data.loc[0, 'R_aftercharg'] == 1:
    #     after.append(0)
    # for i in range(len(data) - 1):
    #     if data.loc[i, 'R_aftercharg'] != data.loc[i + 1, 'R_aftercharg']:
    #         after.append(i + 1)
    # if data.loc[len(data) - 1, 'R_aftercharg'] == 1:
    #     after.append(len(data) - 1)
    #
    # end_aftercharg = []
    # for i in range(len(after) - 1):
    #     if data.loc[after[i], 'R_aftercharg'] == 1:
    #         end_aftercharg.append(after[i + 1] - 1)
    #
    # # ì¢…ë£Œì  í’ˆì§ˆ í•„í„°
    # remove_end = []
    # for i in range(len(end_aftercharg)):
    #     if data.loc[end_aftercharg[i] - 1, 'soc'] < 80:
    #         remove_end.append(end_aftercharg[i])
    # end_aftercharg = [idx for idx in end_aftercharg if idx not in remove_end]

    # â”€ ë§¤ì¹­(ë„˜íŒŒì´)
    dfc_events = []
    if len(charg_2_pairs) and ('R_charg' in data.columns) and ('R_aftercharg' in data.columns):
        ch = data['R_charg'].fillna(0).astype(int).to_numpy()
        ac = data['R_aftercharg'].fillna(0).astype(int).to_numpy()

        # aftercharge ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘/ë
        transitions_ac = np.diff(np.r_[0, ac, 0])        # +1: start, -1: end+1
        astarts = np.where(transitions_ac == +1)[0]
        aends   = np.where(transitions_ac == -1)[0] - 1

        # ì¶©ì „ ì‹œì‘ ì¸ë±ìŠ¤ë“¤
        transitions_ch = np.diff(np.r_[0, ch])           # +1: start
        cstarts = np.where(transitions_ch == +1)[0]

        astarts.sort(); aends.sort(); cstarts.sort()

        t_margin = pd.Timedelta(hours=1)

        for dstart, cend in charg_2_pairs:
            # ğŸ”¹ ì§€ì—° ì‹œì‘ì  SOCê°€ 95 ì´ìƒì´ë©´ ì´ ì´ë²¤íŠ¸ëŠ” DFC ì ìš©í•˜ì§€ ì•ŠìŒ
            if 'soc' in data.columns:
                soc_d = pd.to_numeric(data.loc[dstart, 'soc'], errors='coerce')
                if pd.notna(soc_d) and soc_d >= 95:
                    continue

            # ë‹¤ìŒ ì¶©ì „ ì‹œì‘
            pos_c = np.searchsorted(cstarts, cend + 1, side='left')
            next_charge_start = cstarts[pos_c] if pos_c < len(cstarts) else None

            # cend ì´ìƒì—ì„œ ì‹œì‘í•˜ëŠ” ì²« aftercharge
            pos_a = np.searchsorted(astarts, cend, side='left')
            if pos_a >= len(astarts):
                continue
            astart = astarts[pos_a]

            # ë‹¤ìŒ ì¶©ì „ ì‹œì‘ ì „ì´ì–´ì•¼ í•¨
            if (next_charge_start is not None) and (astart >= next_charge_start):
                continue

            aend = aends[pos_a]  # ë™ì¼ ì„¸ê·¸ë¨¼íŠ¸ ë

            # ì‹œê°„ ê³„ì‚°/ì ìš©
            t0 = data.loc[cend, 'time']
            t1 = data.loc[aend, 'time']
            if pd.isna(t0) or pd.isna(t1):
                continue

            delayed_time = (t1 - t0 - t_margin)
            if (delayed_time > pd.Timedelta(0)) and (dstart + 1 <= cend):
                # ì´ë²¤íŠ¸ ìˆ˜ì§‘
                dfc_events.append({
                    'charge_end_idx': int(cend),
                    'after_end_idx': int(aend),
                    'charge_end_time': t0,
                    'after_end_time': t1,
                    'delay_hours': delayed_time.total_seconds() / 3600.0
                })
                # ì‹¤ì œ ë³´ì • ì ìš©
                data.loc[dstart + 1: cend, 'time'] = data.loc[dstart + 1: cend, 'time'] + delayed_time


    # ì„¸ë¶„í™” ì»¬ëŸ¼ ì‚­ì œ(ì¡´ì¬í•  ë•Œë§Œ)
    columns_to_delete = ['R_charg', 'R_partial_charg', 'R_aftercharg', 'R_uncharg']
    data = data.drop(columns=[c for c in columns_to_delete if c in data.columns], errors='ignore')

    if not collect_stats:
        return data

    # â”€ íŒŒì¼ ë‹¨ìœ„ ìš”ì•½ í†µê³„ (delta_t95_event ë„¤ì´ë° ê³ ì •)
    delays = pd.to_numeric(pd.Series([e['delay_hours'] for e in dfc_events], dtype='float64'), errors='coerce').dropna()
    N = int(len(delays))
    mean = float(delays.mean()) if N > 0 else 0.0
    std  = float(delays.std(ddof=1)) if N > 1 else 0.0
    summ = float(delays.sum()) if N > 0 else 0.0

    stats = {
        'delta_t95_event_N': N,
        'delta_t95_event_mean_h': mean,
        'delta_t95_event_std_h': std,
        'delta_t95_event_sum_h': summ
    }
    return data, dfc_events, stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŒŒì¼ í•˜ë‚˜ ëŒë¦¬ê¸° (ì €ì¥ on/off + ìš”ì•½ ë¦¬í„´)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_DFC_file(file_path, save_path=None, collect_stats=True, write_output=True):
    """
    write_output=False ì´ë©´ ë³€í™˜ëœ DFC CSVë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  í†µê³„ë§Œ ë°˜í™˜.
    """
    data = pd.read_csv(file_path)
    if collect_stats:
        result = DFC(data, collect_stats=True)
        data = result[0]
        _events = result[1]
        stats = result[2]
    else:
        data = DFC(data, collect_stats=False)
        _events, stats = [], None

    if write_output:
        if save_path is None:
            base, ext = os.path.splitext(file_path)
            save_path = f"{base.rstrip('_r')}_DFC{ext}"
        data.to_csv(save_path, index=False)

    return data, stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìš”ì•½ CSV (delta_t95_event ë„¤ì´ë° ê³ ì •)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUMMARY_COLUMNS = [
    'file_stem', 'id_token', 'ym',
    'delta_t95_event_N',
    'delta_t95_event_mean_h',
    'delta_t95_event_std_h',
    'delta_t95_event_sum_h',
]

def parse_id_token_and_ym(p: Path):
    # íŒŒì¼ëª… ì˜ˆ: bms_01241228021_2023-02_r.csv â†’ id_token=01241228021, ym=2023-02
    id_token, ym = "unknown", "0000-00"
    parts = p.stem.split("_")
    if len(parts) >= 3:
        id_token = parts[1]
        ym = parts[2]
    return id_token, ym

def _collect_input_files(input_folder, pattern="*.csv"):
    input_dir = Path(input_folder)
    files = [p for p in sorted(input_dir.glob(pattern))]
    return files

def process_DFC_folder(input_folder, output_folder, summary_csv_path=None,
                       pattern="*.csv", write_outputs=True, skip_existing=True):
    files = _collect_input_files(input_folder, pattern=pattern)
    return _process_files_and_summary(files, output_folder, summary_csv_path,
                                      write_outputs=write_outputs, skip_existing=skip_existing)


def process_DFC_folder_slice(input_folder, output_folder, start_idx=0, end_idx=None,
                             summary_csv_path=None, pattern="*.csv", write_outputs=True,
                             skip_existing=True):
    files = _collect_input_files(input_folder, pattern=pattern)
    if end_idx is None:
        sel = files[start_idx:]
    else:
        sel = files[start_idx:end_idx+1]  # inclusive
    return _process_files_and_summary(sel, output_folder, summary_csv_path,
                                      write_outputs=write_outputs, skip_existing=skip_existing)


def _process_files_and_summary(files, output_folder, summary_csv_path=None,
                               write_outputs=True, skip_existing=True):
    output_dir = Path(output_folder)
    if write_outputs:
        output_dir.mkdir(parents=True, exist_ok=True)
    summary_rows = []

    for p in tqdm(files, desc='Processing Files'):
        try:
            save_path = None
            if write_outputs:
                out_name = p.name.replace('_r.csv', '_DFC.csv')
                save_path = output_dir / out_name

                # â”€â”€ ì—¬ê¸°! ê²°ê³¼ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                if skip_existing and save_path.exists():
                    # í•„ìš”í•˜ë©´ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ìš”ì•½ì—ì„œëŠ” ì œì™¸(ë¹ ë¥´ê²Œ ëŒë¦¬ê¸° ëª©ì )
                    # tqdm.write(f"[skip] {out_name}")
                    continue
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            _, stats = process_DFC_file(
                str(p),
                save_path=str(save_path) if save_path else None,
                collect_stats=True,
                write_output=write_outputs
            )

            # ìš”ì•½ ìˆ˜ì§‘
            id_token, ym = parse_id_token_and_ym(p)
            if stats is None:
                stats = {
                    'delta_t95_event_N': 0,
                    'delta_t95_event_mean_h': 0.0,
                    'delta_t95_event_std_h': 0.0,
                    'delta_t95_event_sum_h': 0.0
                }

            summary_rows.append({
                'file_stem': p.stem,
                'id_token': id_token,
                'ym': ym,
                'delta_t95_event_N': stats['delta_t95_event_N'],
                'delta_t95_event_mean_h': stats['delta_t95_event_mean_h'],
                'delta_t95_event_std_h': stats['delta_t95_event_std_h'],
                'delta_t95_event_sum_h': stats['delta_t95_event_sum_h'],
            })

        except Exception as e:
            print(f"Error processing {p.name}: {str(e)}")
            id_token, ym = parse_id_token_and_ym(p)
            summary_rows.append({
                'file_stem': p.stem,
                'id_token': id_token,
                'ym': ym,
                'delta_t95_event_N': 0,
                'delta_t95_event_mean_h': 0.0,
                'delta_t95_event_std_h': 0.0,
                'delta_t95_event_sum_h': 0.0,
            })
            continue

    # ìš”ì•½ CSV ì €ì¥
    if summary_csv_path is None:
        summary_csv_path = Path(output_folder) / "dfc_summary.csv"
    summary_df = pd.DataFrame(summary_rows, columns=SUMMARY_COLUMNS)
    summary_df.to_csv(summary_csv_path, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ìš”ì•½ ì €ì¥: {summary_csv_path} (rows={len(summary_df)})")

    return summary_df


def _dfc_one_file_job(args):
    """
    (in_path, out_path_or_None, write_outputs, skip_existing) ë°›ì•„ì„œ
    - DFC ì²˜ë¦¬(ì˜µì…˜ìœ¼ë¡œ íŒŒì¼ ì €ì¥)
    - stats dict ë°˜í™˜ (ìš”ì•½ìš©)
    """
    in_path, out_path, write_outputs, skip_existing = args
    p = Path(in_path)

    # ì¶œë ¥ íŒŒì¼ì´ ìˆê³  ìŠ¤í‚µì´ë©´: í†µê³„ë„ ìŠ¤í‚µí• ì§€ ì •ì±… ì„ íƒ í•„ìš”
    # ì§€ê¸ˆì€ "ìŠ¤í‚µë˜ë©´ ìš”ì•½ì—ì„œë„ ì œì™¸" = ê¸°ì¡´ ë‹¨ì¼í”„ë¡œì„¸ìŠ¤ì™€ ë™ì¼ ë™ì‘
    if write_outputs and out_path and skip_existing and Path(out_path).exists():
        return ("skip", p.stem, None)

    try:
        _, stats = process_DFC_file(
            str(p),
            save_path=str(out_path) if (write_outputs and out_path) else None,
            collect_stats=True,
            write_output=write_outputs
        )

        if stats is None:
            stats = {
                'delta_t95_event_N': 0,
                'delta_t95_event_mean_h': 0.0,
                'delta_t95_event_std_h': 0.0,
                'delta_t95_event_sum_h': 0.0
            }

        # ìš”ì•½ row ìƒì„±
        id_token, ym = parse_id_token_and_ym(p)
        row = {
            'file_stem': p.stem,
            'id_token': id_token,
            'ym': ym,
            'delta_t95_event_N': int(stats['delta_t95_event_N']),
            'delta_t95_event_mean_h': float(stats['delta_t95_event_mean_h']),
            'delta_t95_event_std_h': float(stats['delta_t95_event_std_h']),
            'delta_t95_event_sum_h': float(stats['delta_t95_event_sum_h']),
        }
        return ("ok", p.stem, row)

    except Exception as e:
        # ì—ëŸ¬ë„ ìš”ì•½ì— 0ìœ¼ë¡œ ë‚¨ê¹€(ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        id_token, ym = parse_id_token_and_ym(p)
        row = {
            'file_stem': p.stem,
            'id_token': id_token,
            'ym': ym,
            'delta_t95_event_N': 0,
            'delta_t95_event_mean_h': 0.0,
            'delta_t95_event_std_h': 0.0,
            'delta_t95_event_sum_h': 0.0,
        }
        return ("error", f"{p.name}: {e}", row)


def process_DFC_folder_mp(
    input_folder,
    output_folder,
    summary_csv_path=None,
    pattern="*.csv",
    write_outputs=True,
    skip_existing=True,
    workers=None,
):
    """
    DFC ë©€í‹°í”„ë¡œì„¸ìŠ¤ í´ë” ì²˜ë¦¬ + summary ìƒì„±
    - write_outputs=True : *_DFC.csv ì €ì¥
    - skip_existing=True : *_DFC.csv ìˆìœ¼ë©´ ìŠ¤í‚µ(ìš”ì•½ì—ì„œë„ ì œì™¸: ê¸°ì¡´ê³¼ ë™ì¼)
    """
    files = _collect_input_files(input_folder, pattern=pattern)
    if not files:
        print("[info] ì²˜ë¦¬í•  CSVê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=SUMMARY_COLUMNS)

    output_dir = Path(output_folder)
    if write_outputs:
        output_dir.mkdir(parents=True, exist_ok=True)

    # ì‘ì—… ë¦¬ìŠ¤íŠ¸
    jobs = []
    for p in files:
        out_path = None
        if write_outputs:
            out_name = p.name.replace("_r.csv", "_DFC.csv")
            out_path = str(output_dir / out_name)
        jobs.append((str(p), out_path, write_outputs, skip_existing))

    summary_rows = []
    ok = skip = err = 0

    with ProcessPoolExecutor(max_workers=workers) as ex:
        futures = [ex.submit(_dfc_one_file_job, job) for job in jobs]
        for fut in tqdm(as_completed(futures), total=len(futures), desc="DFC Processing (MP)"):
            status, msg, row = fut.result()
            if status == "ok":
                ok += 1
                summary_rows.append(row)
            elif status == "skip":
                skip += 1
                # ìŠ¤í‚µì€ ìš”ì•½ ì œì™¸(ê¸°ì¡´ê³¼ ë™ì¼)
            else:
                err += 1
                print(f"[error] {msg}")
                summary_rows.append(row)

    # summary ì €ì¥
    if summary_csv_path is None:
        summary_csv_path = str(output_dir / "dfc_summary.csv") if write_outputs else str(Path(input_folder) / "dfc_summary.csv")

    summary_df = pd.DataFrame(summary_rows, columns=SUMMARY_COLUMNS)
    summary_df.to_csv(summary_csv_path, index=False, encoding="utf-8-sig")

    print(f"[done] ok={ok}, skip={skip}, error={err}")
    print(f"âœ… ìš”ì•½ ì €ì¥: {summary_csv_path} (rows={len(summary_df)})")
    return summary_df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰ ì˜ˆì‹œ(í•„ìš”í•œ ë¶€ë¶„ë§Œ ì£¼ì„ í•´ì œí•´ì„œ ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    # ê²½ë¡œ ì„¤ì •
    # input_folder_path = r'Z:\SamsungSTF\Processed_Data\DFC\EV6\R_parsing_ì›ë³¸'
    # output_folder_path = r'Z:\SamsungSTF\Processed_Data\DFC\EV6\DFC_ì›ë³¸'
    # summary_folder_path = r'G:\ê³µìœ  ë“œë¼ì´ë¸Œ\BSG_DFC_result\EV6\DFC_ì›ë³¸'
    # summary_csv_path = os.path.join(summary_folder_path, 'dfc_features_summary.csv')
    # os.makedirs(summary_folder_path, exist_ok=True)

    input_folder_path = r'C:\Users\junny\SynologyDrive\SamsungSTF\Processed_Data\DFC\EV6\R_parsing_ì™„ì¶©í›„ì´ë™ì£¼ì°¨'
    output_folder_path = r'C:\Users\junny\SynologyDrive\SamsungSTF\Processed_Data\DFC\EV6\DFC_ì™„ì¶©í›„ì´ë™ì£¼ì°¨'
    summary_folder_path = r'C:\Users\junny\SynologyDrive\SamsungSTF\Processed_Data\DFC\EV6\DFC_ìˆ˜ì •ìš©_251202'
    summary_csv_path = os.path.join(summary_folder_path, 'dfc_features_summary.csv')
    os.makedirs(summary_folder_path, exist_ok=True)

    # â‘£ íŒŒì¼ í•˜ë‚˜ë§Œ ëŒë¦¬ê¸° (ì €ì¥ O, í†µê³„ í™•ì¸)
    file_name = "bms_01241228082_2023-10_r.csv"  # â† ì—¬ê¸°ë§Œ ë°”ê¿”ì£¼ë©´ ë¨
    file_path = os.path.join(input_folder_path, file_name)
    save_path = os.path.join(output_folder_path, file_name.replace('_r.csv', '_DFC.csv'))

    processed_df, stats = process_DFC_file(
        file_path,
        save_path=save_path,
        collect_stats=True,  # í†µê³„ë„ ë³´ê³  ì‹¶ìœ¼ë©´ True
        write_output=True  # DFC CSV ì €ì¥í•˜ê³  ì‹¶ìœ¼ë©´ True
    )

    print(stats)

    # # â‘  í´ë” ì „ì²´ ëŒë¦¬ê¸° (íŒŒì¼ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœ) + ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
    # process_DFC_folder(
    #     input_folder_path,
    #     output_folder_path,
    #     summary_csv_path=summary_csv_path,
    #     pattern="*.csv",
    #     write_outputs=True,
    #     skip_existing=True   # â† ê²°ê³¼ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
    # )

    # # â‘¡ í´ë” ì „ì²´ "ìš”ì•½ë§Œ" ìƒì„± (ê°œë³„ íŒŒì¼ ì €ì¥ X) â€” ìŠ¤í‚µ ì˜µì…˜ì€ ì˜í–¥ ì ìŒ
    # process_DFC_folder(
    #     input_folder_path,
    #     output_folder_path,
    #     summary_csv_path=summary_csv_path,
    #     pattern="*.csv",
    #     write_outputs=False,
    #     skip_existing=False
    # )

    # â‘¢ ì‹œì‘~ë ì¸ë±ìŠ¤ë¡œ ë‚˜ëˆ ì„œ ëŒë¦¬ê¸° (ì´ë¦„ìˆœ, end_idx í¬í•¨) + ìŠ¤í‚µ
    # process_DFC_folder_slice(
    #     input_folder_path,
    #     output_folder_path,
    #     start_idx=644,
    #     end_idx=766,  # â† 0~9 ì´ 10ê°œ
    #     summary_csv_path=summary_csv_path,
    #     pattern="*.csv",
    #     write_outputs=True,
    #     skip_existing=True
    # )


    # # ë©€í‹°í”„ë¡œì„¸ìŠ¤
    # process_DFC_folder_mp(
    #     input_folder_path,
    #     output_folder_path,
    #     summary_csv_path=summary_csv_path,
    #     pattern="*.csv",
    #     write_outputs=True,
    #     skip_existing=True,
    #     workers=8,   # ë„¤íŠ¸ì›Œí¬ ë“œë¼ì´ë¸Œë©´ 4~8 ì¶”ì²œ
    # )


